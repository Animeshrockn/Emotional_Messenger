# Emotional_messenger
Text-to-Speech conversion using textual and facial emotion detection

Our project will combine facial and textual recognition models of emotion recognition to analyze a text and convert it into to its emotion recognised speech. Generally, a tone of the text we write is lagging emotions, we will try to combine facial expressions with the typed text to understand a particular emotion that is depicted in the text and voice out the text using the recognised emotion. Our focus is on real-time prediction of a personâ€™s emotion while typing with a good accuracy. Initially we will concentrate on 4 emotions: anger, happiness, sadness and neutral but eventually we are planning to incorporate 7 emotions: anger, happiness, sadness, surprise, disgust, fear and neutral. It has three components: (i) A face recognition model for emotions (Detecting the 4 emotions as mentioned above) (ii) Text Recognition model for emotions (Detecting the 4 emotions as mentioned above) (iii) Text to speech software that has a collection of different tones for emotions. The average of the two recognition models will give our approximate emotion that will set the tone for text to speech. The task seems to be challenging since it is composed of three parts that interact with each-other. The challenges we might face include, difference in textual recognition and facial recognition for emotions, availability of exact tones for text to speech. We believe our project might be of great help that would provide an actual tone to the texts in real time, especially for people with visibility disabilities. It would be pretty useful in general while texting when we are not able to understand the tone of the texts. All in all, we found it to be a fun experiment to understand the depth of emotions in textual and facial forms.

![alt text](https://github.com/animeshrockn/Emotional_messenger/blob/master/bb.png?raw=true)
