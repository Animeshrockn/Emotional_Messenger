{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"olGQU0bPQhnP","executionInfo":{"status":"ok","timestamp":1649956657942,"user_tz":420,"elapsed":1124,"user":{"displayName":"Animesh","userId":"12700777712753097964"}},"outputId":"c10d5d90-e5d9-412c-c4bf-44571fb3e764"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ctIIohOsNRW4","executionInfo":{"status":"ok","timestamp":1649956702994,"user_tz":420,"elapsed":45060,"user":{"displayName":"Animesh","userId":"12700777712753097964"}},"outputId":"0a682abd-0f47-4c45-f144-64e9d1844d2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tweet-preprocessor\n","  Downloading tweet_preprocessor-0.6.0-py3-none-any.whl (27 kB)\n","Installing collected packages: tweet-preprocessor\n","Successfully installed tweet-preprocessor-0.6.0\n","Collecting emoji\n","  Downloading emoji-1.7.0.tar.gz (175 kB)\n","\u001b[K     |████████████████████████████████| 175 kB 5.1 MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=0bd3a6eb02337b3956cd4f36af3c00f8d865ec07b32a5ac8d5d9243f982cf6e6\n","  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-1.7.0\n","Collecting transformers\n","  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n","\u001b[K     |████████████████████████████████| 4.0 MB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 42.9 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 4.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 35.1 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 42.5 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n","/bin/bash: conda: command not found\n","Collecting plotly-express\n","  Downloading plotly_express-0.4.1-py2.py3-none-any.whl (2.9 kB)\n","Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.7/dist-packages (from plotly-express) (1.4.1)\n","Requirement already satisfied: plotly>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from plotly-express) (5.5.0)\n","Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from plotly-express) (0.10.2)\n","Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.7/dist-packages (from plotly-express) (0.5.2)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from plotly-express) (1.21.5)\n","Requirement already satisfied: pandas>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from plotly-express) (1.3.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.0->plotly-express) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.0->plotly-express) (2018.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5->plotly-express) (1.15.0)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=4.1.0->plotly-express) (8.0.1)\n","Installing collected packages: plotly-express\n","Successfully installed plotly-express-0.4.1\n","Collecting pyyaml==5.4.1\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 5.0 MB/s \n","\u001b[?25hInstalling collected packages: pyyaml\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 6.0\n","    Uninstalling PyYAML-6.0:\n","      Successfully uninstalled PyYAML-6.0\n","Successfully installed pyyaml-5.4.1\n"]}],"source":["# Installing Dependencies\n","!pip install tweet-preprocessor                                                   # https://github.com/s/preprocessor\n","!pip install emoji                                                                # https://github.com/carpedm20/emoji/\n","!pip install transformers                                                         # https://github.com/huggingface/transformers\n","!conda install -c saidozcan tweet-preprocessor                                    # Installation of tweet-preprocessor for Conda env\n","!pip install plotly-express                                                       # https://plotly.com/python/plotly-express/\n","!pip install pyyaml==5.4.1                                                        # Plotly-express installs pyyaml new version so we need to downgrade it for plotly to run on collab\n","                                                                                  # Once installed we need to just restart the kernel once and we can start importing thereafter"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":14057,"status":"ok","timestamp":1649956717045,"user":{"displayName":"Animesh","userId":"12700777712753097964"},"user_tz":420},"id":"j-UJfVXE_kC4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e259962c-77b0-4996-f383-a4138020f7d7"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n","  defaults = yaml.load(f)\n"]}],"source":["import preprocessor as p                                                          # https://github.com/s/preprocessor\n","import numpy as np                                                                # https://numpy.org\n","import pandas as pd                                                               # https://pandas.pydata.org\n","import emoji                                                                      # https://github.com/carpedm20/emoji/\n","import keras                                                                      # https://keras.io\n","from sklearn.model_selection import train_test_split                              # https://scikit-learn.org/stable/\n","import tensorflow as tf                                                           # https://www.tensorflow.org\n","from keras.models import Sequential\n","from keras.layers.recurrent import LSTM, GRU,SimpleRNN\n","from keras.layers.core import Dense, Activation, Dropout\n","from keras.layers.embeddings import Embedding\n","from keras.layers import BatchNormalization\n","from keras.utils import np_utils\n","from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n","from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n","from keras.preprocessing import sequence, text\n","from keras.callbacks import EarlyStopping\n","from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n","import plotly.graph_objects as go                                                 # https://plotly.com\n","import plotly.express as px\n","from tensorflow.keras.layers import Dense, Input\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","import transformers                                                               # https://huggingface.co/docs/transformers/index\n","from transformers import TFAutoModel, AutoTokenizer\n","from tqdm.notebook import tqdm\n","from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt                                                   # https://matplotlib.org\n","import plotly.graph_objects as go\n","import seaborn as sns                                                             # https://seaborn.pydata.org\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.models import model_from_json\n","import tensorflow as tf\n","from sklearn.metrics import confusion_matrix\n","\n","                  \n","\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","source":["## Phase I - Text Analysis using Twitter Dataset"],"metadata":{"id":"O5YKPr6s3yE5"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"p8nNVXDg-GXz","executionInfo":{"status":"ok","timestamp":1649956717933,"user_tz":420,"elapsed":890,"user":{"displayName":"Animesh","userId":"12700777712753097964"}}},"outputs":[],"source":["# Twitter Dataset for training Text Analysis Model \n","text_data = pd.read_csv('/content/drive/MyDrive/Emotional_messenger/text/text_emotion.csv')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"WFvEC5neNRXa","executionInfo":{"status":"ok","timestamp":1649956718096,"user_tz":420,"elapsed":166,"user":{"displayName":"Animesh","userId":"12700777712753097964"}}},"outputs":[],"source":["# Checking Misspelled words in data\n","\n","misspell_data = pd.read_csv(\"/content/drive/MyDrive/Emotional_messenger/text/aspell.txt\",sep=\":\",names=[\"correction\",\"misspell\"]) #Text document with some major used misspelled dataset to replace https://www.kaggle.com/datasets/bittlingmayer/spelling\n","misspell_data.misspell = misspell_data.misspell.str.strip()                       # Stripping spaces from starting and ending\n","misspell_data.misspell = misspell_data.misspell.str.split(\" \")                    # Splitting words by space\n","misspell_data = misspell_data.explode(\"misspell\").reset_index(drop=True)          # Dropping the misspell and adjusting the index\n","misspell_data.drop_duplicates(\"misspell\",inplace=True)                            # Duplicate removla\n","miss_corr = dict(zip(misspell_data.misspell, misspell_data.correction))           # Putting all in a dict to replace word by word in dataset\n","# miss_corr"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4XbTys-NRXd","executionInfo":{"status":"ok","timestamp":1649956718096,"user_tz":420,"elapsed":6,"user":{"displayName":"Animesh","userId":"12700777712753097964"}},"outputId":"14f5c813-dee1-4fd4-eb77-ae295f07418a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'Steffen': 'Stephen',\n"," 'nevade': 'Nevada',\n"," 'presbyterian': 'Presbyterian',\n"," 'rsx': 'RSX',\n"," 'susan': 'Susan'}"]},"metadata":{},"execution_count":6}],"source":["# Printing first 5 values from miss_corr for display\n","\n","f5p = {k: miss_corr[k] for k in list(miss_corr)[:5]}\n","f5p"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"wcFZqK86NRXi","executionInfo":{"status":"ok","timestamp":1649956718285,"user_tz":420,"elapsed":191,"user":{"displayName":"Animesh","userId":"12700777712753097964"}}},"outputs":[],"source":["# Creating a function to replace all the words from dataset with misspelled words dict created above (miss_corr)\n","\n","def misspelled_correction(val):\n","    for x in val.split(): \n","        if x in miss_corr.keys(): \n","            val = val.replace(x, miss_corr[x]) \n","    return val\n","\n","text_data[\"clean_content\"] = text_data.content.apply(lambda x : misspelled_correction(x))"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"DowEOxMFNRXm","executionInfo":{"status":"ok","timestamp":1649956718636,"user_tz":420,"elapsed":354,"user":{"displayName":"Animesh","userId":"12700777712753097964"}}},"outputs":[],"source":["# Same process for replacing all the contactions using contraction dataset \n","\n","contractions = pd.read_csv(\"/content/drive/MyDrive/Emotional_messenger/text/contractions.csv\")\n","cont_dic = dict(zip(contractions.Contraction, contractions.Meaning))             # Moving all entries to a dict to replace in data"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XsOovFTRNRXo","executionInfo":{"status":"ok","timestamp":1649956718637,"user_tz":420,"elapsed":11,"user":{"displayName":"Animesh","userId":"12700777712753097964"}},"outputId":"3069a20e-52ed-4509-d314-14fe92e5759c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{\"'aight\": 'alright',\n"," \"ain't\": 'is not',\n"," \"amn't\": 'am not',\n"," \"aren't\": 'are not',\n"," \"can't\": 'cannot'}"]},"metadata":{},"execution_count":9}],"source":["# Printing first 5 values from cont_dic for display\n","\n","f5p = {k: cont_dic[k] for k in list(cont_dic)[:5]}\n","f5p "]},{"cell_type":"code","execution_count":10,"metadata":{"id":"fiCbmOjqNRXr","executionInfo":{"status":"ok","timestamp":1649956718850,"user_tz":420,"elapsed":218,"user":{"displayName":"Animesh","userId":"12700777712753097964"}}},"outputs":[],"source":["# Creating a function to replace all the words from dataset with misspelled words dataset created above (cont_dic)\n","\n","def cont_to_meaning(val): \n","  \n","    for x in val.split(): \n","        if x in cont_dic.keys(): \n","            val = val.replace(x, cont_dic[x]) \n","    return val\n","\n","text_data.clean_content = text_data.clean_content.apply(lambda x : cont_to_meaning(x))"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"imnz01JtNRXu","executionInfo":{"status":"ok","timestamp":1649956723281,"user_tz":420,"elapsed":4435,"user":{"displayName":"Animesh","userId":"12700777712753097964"}},"outputId":"b14012cd-9f1d-4cbd-d7b0-5209010b78c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Printing example of Preprocessor below:\n","\n","Origional string : Affective Computing @Angelica #machine_learning CMPT724 https://canvas.sfu.ca/courses/68760\n","Converted to : Affective Computing #machine_learning CMPT724\n"]}],"source":["# Removing Hyperlink\n","\n","p.set_options(p.OPT.MENTION, p.OPT.URL)\n","print(\"Printing example of Preprocessor below:\\n\")\n","print(\"Origional string : Affective Computing @Angelica #machine_learning CMPT724 https://canvas.sfu.ca/courses/68760\")\n","print(f'Converted to : {p.clean(\"Affective Computing @Angelica #machine_learning CMPT724 https://canvas.sfu.ca/courses/68760\")}')\n","text_data[\"clean_content\"]=text_data.content.apply(lambda x : p.clean(x))"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"yidtCzl8NRXx","executionInfo":{"status":"ok","timestamp":1649956723282,"user_tz":420,"elapsed":8,"user":{"displayName":"Animesh","userId":"12700777712753097964"}}},"outputs":[],"source":["# Removing Punctuation form Regex Entry replacement into the dataset\n","\n","def punctuation(val): \n","  \n","    punctuations = '''()-[]{};:'\"\\,<>./@#$%^&_~'''\n","  \n","    for x in val.lower(): \n","        if x in punctuations: \n","            val = val.replace(x, \" \") \n","    return val"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OKlmL3HiNRX1","executionInfo":{"status":"ok","timestamp":1649956723282,"user_tz":420,"elapsed":7,"user":{"displayName":"Animesh","userId":"12700777712753097964"}},"outputId":"ee61e51d-26a5-4a55-b643-b979ae76a8c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Origional Text :  @ () {} [] <> ldfldlf???~~~ !! \n","Converted to :                ldfldlf???    !! \n"]}],"source":["# Testing Punctuation function \n","\n","print(\"Origional Text :  @ () {} [] <> ldfldlf???~~~ !! \") \n","print(f'Converted to : {punctuation(\"@ () {} [] <>  ldfldlf???~~~ !! \")}')"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"hnTKnxI4NRX3","executionInfo":{"status":"ok","timestamp":1649956726236,"user_tz":420,"elapsed":2958,"user":{"displayName":"Animesh","userId":"12700777712753097964"}}},"outputs":[],"source":["# Removing Emojis from the dataset https://pypi.org/project/emoji/#description \n","\n","import emoji\n","text_data.clean_content = text_data.clean_content.apply(lambda x : ' '.join(punctuation(emoji.demojize(x)).split()))"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"ATIT7J10NRX4","executionInfo":{"status":"ok","timestamp":1649956726239,"user_tz":420,"elapsed":18,"user":{"displayName":"Animesh","userId":"12700777712753097964"}}},"outputs":[],"source":["# Putting all togeather in one function to be used before tokenization\n","\n","def clean_text(val):\n","    val = misspelled_correction(val)\n","    val = cont_to_meaning(val)\n","    val = p.clean(val)\n","    val = ' '.join(punctuation(emoji.demojize(val)).split())\n","    \n","    return val"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Bru3j1dgNRX6","executionInfo":{"status":"ok","timestamp":1649956726239,"user_tz":420,"elapsed":16,"user":{"displayName":"Animesh","userId":"12700777712753097964"}},"outputId":"fc54b091-bb5e-4868-9f4d-8fd08bd6e45b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'is not light bulb adultery good bad ! ?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}],"source":["# Trying clean_text function \n","\n","clean_text(\"isn't 💡 adultry @ttt good bad ... ! ? \")"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"53CKlT1GNRX7","executionInfo":{"status":"ok","timestamp":1649956726240,"user_tz":420,"elapsed":6,"user":{"displayName":"Animesh","userId":"12700777712753097964"}}},"outputs":[],"source":["# Cleaning any empty comments\n","\n","text_data = text_data[text_data.clean_content != \"\"]"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rL2KOfXcNRX9","executionInfo":{"status":"ok","timestamp":1649956726440,"user_tz":420,"elapsed":206,"user":{"displayName":"Animesh","userId":"12700777712753097964"}},"outputId":"7d9ebca8-00b2-4d2e-dbea-bae3d0ac5488"},"outputs":[{"output_type":"stream","name":"stdout","text":["Occurance of various emotions in the dataset :\n"]},{"output_type":"execute_result","data":{"text/plain":["neutral       8579\n","worry         8454\n","happiness     5208\n","sadness       5162\n","love          3841\n","surprise      2187\n","fun           1776\n","relief        1526\n","hate          1323\n","empty          815\n","enthusiasm     759\n","boredom        179\n","anger          110\n","Name: sentiment, dtype: int64"]},"metadata":{},"execution_count":18}],"source":["# Number of samples for each emotion in dataset\n","\n","print(\"Occurance of various emotions in the dataset :\")\n","text_data['sentiment'].value_counts()\n","# data[data['sentiment'] == 'empty']                # Visualize each emotion with total rows"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"JxjkEwexNRX_","executionInfo":{"status":"ok","timestamp":1649956726440,"user_tz":420,"elapsed":2,"user":{"displayName":"Animesh","userId":"12700777712753097964"}}},"outputs":[],"source":["# Since we need to make a basic 6 emotion model, we are converging some of the secondary emotions to their corresponding emotion\n","# and also we will be removing some of the emotion as they do not have much data available for model to train better.\n","# Our Basic emotions : anger, disgust, surprised, happy, fear, neutral, sad\n","# 1. We are dropping all empty emotions as manually checking the content says that the emotion are mixed and its better to drop it\n","# 2. we will merge worry to sad\n","# 3. change name for happiness to happy\n","# 4. change name for sadness to sad\n","# 5. Merge love to happy\n","# 6. Merge fun to happy\n","# 7. Merge relief to neutral\n","# 8. Merge hate to anger\n","# 9. Drop enthusiasm and boredom \n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"As7GOAZSNRYA","executionInfo":{"status":"ok","timestamp":1649956726590,"user_tz":420,"elapsed":152,"user":{"displayName":"Animesh","userId":"12700777712753097964"}}},"outputs":[],"source":["# Dropping and merging as mentioned above\n","\n","text_data.drop(text_data.loc[text_data['sentiment']=='empty'].index, inplace=True)\n","text_data['sentiment'] = text_data['sentiment'].replace(['worry'],'sad')\n","text_data['sentiment'] = text_data['sentiment'].replace(['happiness'],'happy')\n","text_data['sentiment'] = text_data['sentiment'].replace(['sadness'],'sad')\n","text_data['sentiment'] = text_data['sentiment'].replace(['love'],'happy')\n","text_data['sentiment'] = text_data['sentiment'].replace(['fun'],'happy')\n","text_data['sentiment'] = text_data['sentiment'].replace(['relief'],'neutral')\n","text_data['sentiment'] = text_data['sentiment'].replace(['hate'],'anger')\n","text_data.drop(text_data.loc[text_data['sentiment']=='enthusiasm'].index, inplace=True)\n","text_data.drop(text_data.loc[text_data['sentiment']=='boredom'].index, inplace=True)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdJPupnCNRYC","executionInfo":{"status":"ok","timestamp":1649956726591,"user_tz":420,"elapsed":5,"user":{"displayName":"Animesh","userId":"12700777712753097964"}},"outputId":"523a3409-f6fa-4c19-90a4-b15bcc85fbeb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["sad         13616\n","happy       10825\n","neutral     10105\n","surprise     2187\n","anger        1433\n","Name: sentiment, dtype: int64"]},"metadata":{},"execution_count":21}],"source":["# Checking the count of rows per emotion after merging\n","\n","text_data['sentiment'].value_counts()            "]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"LHVfH81ANRYE","executionInfo":{"status":"ok","timestamp":1649956726971,"user_tz":420,"elapsed":383,"user":{"displayName":"Animesh","userId":"12700777712753097964"}},"outputId":"c04dc906-c25c-49ee-d167-a203f9890292"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# Pie chart representing dataset distribution of labels\n","\n","\n","labels = ['Sad', 'Happy', 'Neutral', 'Surprise','Anger']\n","sizes = [13616, 10825, 10105, 2187, 1433]\n","\n","#colors\n","colors = ['#ff9999','#66b3ff','#99ff99','#ffcc99', '#c2c2f0']\n","\n","#explsion\n","explode = (0.05,0.05,0.05,0.05,0.05)\n","\n","plt.pie(sizes, colors = colors, labels=labels, autopct='%1.1f%%', startangle=90, pctdistance=0.85, explode = explode)\n","\n","#draw circle\n","centre_circle = plt.Circle((0,0),0.70,fc='white')\n","fig = plt.gcf()\n","fig.gca().add_artist(centre_circle)\n","\n","# Equal aspect ratio ensures that pie is drawn as a circle\n","\n","plt.title(\"Distribution of Mutliple labels in Dataset \" ,fontweight='bold')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","source":["# Function for implementing model on single sentence input\n","\n","def get_sentiment(model,text):\n","    text = clean_text(text)\n","    #tokenize\n","    twt = token.texts_to_sequences([text])\n","    twt = sequence.pad_sequences(twt, maxlen=max_len, dtype='int32')\n","    sentiment = model.predict(twt,batch_size=1,verbose = 2)\n","    sent = np.round(np.dot(sentiment,100).tolist(),0)[0]\n","    result = pd.DataFrame([sent_to_id.keys(),sent]).T\n","    result.columns = [\"sentiment\",\"percentage\"]\n","    result=result[result.percentage != None]\n","\n","    return result"],"metadata":{"id":"sLm867E4Xa5t","executionInfo":{"status":"ok","timestamp":1649956726972,"user_tz":420,"elapsed":5,"user":{"displayName":"Animesh","userId":"12700777712753097964"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Function for Plotting \n","\n","\n","def plot_result(df):\n","    #colors=['#D50000','#000000','#008EF8','#F5B27B','#EDECEC','#D84A09','#019BBD','#FFD000','#7800A0','#098F45','#807C7C','#85DDE9','#F55E10']\n","\n","    labels = ['Sad', 'Happy', 'Neutral', 'Surprise','Anger']\n","    colors={'sad':'rgb(64,224,208)','happy':'rgb(100,149,237)',\n","                    'neutral':'rgb(138,43,226)','surprise':'rgb(221,160,221)',\n","                    'anger':'rgb(255,218,185)'}\n","    col_2={}\n","    for i in result.sentiment.to_list():\n","        col_2[i]=colors[i]\n","    fig = px.pie(df, values='percentage', names='sentiment',color='sentiment',color_discrete_map=col_2,hole=0.3)\n","\n","    fig.show()"],"metadata":{"id":"4_gGUuZHXdvD","executionInfo":{"status":"ok","timestamp":1649956726972,"user_tz":420,"elapsed":4,"user":{"displayName":"Animesh","userId":"12700777712753097964"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x45kXCB0NRYG"},"source":["#### Encoding and Splitting the dataset  "]},{"cell_type":"code","execution_count":25,"metadata":{"id":"6AQcYmkUNRYQ","executionInfo":{"status":"ok","timestamp":1649956726972,"user_tz":420,"elapsed":4,"user":{"displayName":"Animesh","userId":"12700777712753097964"}}},"outputs":[],"source":["# Label Encoding Manually \n","sent_to_id  = {\"sad\":0, \"happy\":1,\"neutral\":2,\"surprise\":3,\"anger\":4}"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"r4gfQsUlNRYV","executionInfo":{"status":"ok","timestamp":1649956726973,"user_tz":420,"elapsed":5,"user":{"displayName":"Animesh","userId":"12700777712753097964"}}},"outputs":[],"source":["# Creating column for label encoded data\n","text_data[\"sentiment_id\"] = text_data['sentiment'].map(sent_to_id)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"HZIlF3fJNRYX","executionInfo":{"status":"ok","timestamp":1649956726973,"user_tz":420,"elapsed":4,"user":{"displayName":"Animesh","userId":"12700777712753097964"}}},"outputs":[],"source":["# One hot encoding the sentiment ID column\n","from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n","\n","label_encoder = LabelEncoder()\n","integer_encoded = label_encoder.fit_transform(text_data.sentiment_id)\n","\n","onehot_encoder = OneHotEncoder(sparse=False)\n","integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n","Y = onehot_encoder.fit_transform(integer_encoded)"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"ArWC7C_aNRYZ","executionInfo":{"status":"ok","timestamp":1649956727214,"user_tz":420,"elapsed":245,"user":{"displayName":"Animesh","userId":"12700777712753097964"}}},"outputs":[],"source":["# Test Train Shuffle for training\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(text_data.clean_content,Y, random_state=1995, test_size=0.15, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"TBBbJKOjNRYa"},"source":["## LSTM Model"]},{"cell_type":"markdown","metadata":{"id":"dnzW9l7SNRYa"},"source":["##### NOTE : Model training and related fileds are commented out since we saved the model and using saved model for calculations"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"5UDc3xjcNRYb","executionInfo":{"status":"ok","timestamp":1649956730477,"user_tz":420,"elapsed":3266,"user":{"displayName":"Animesh","userId":"12700777712753097964"}}},"outputs":[],"source":["# Building LSTM Model for the text model\n","\n","# Using keras tokenizer here for converting dataset to vector\n","token = text.Tokenizer(num_words=None)\n","max_len = 160\n","Epoch = 10\n","batch_size = 32\n","token.fit_on_texts(list(X_train) + list(X_test))\n","X_train_pad = sequence.pad_sequences(token.texts_to_sequences(X_train), maxlen=max_len)\n","X_test_pad = sequence.pad_sequences(token.texts_to_sequences(X_test), maxlen=max_len)\n","w_idx = token.word_index"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-HDBBlThNRYc","executionInfo":{"status":"ok","timestamp":1649956731230,"user_tz":420,"elapsed":764,"user":{"displayName":"Animesh","userId":"12700777712753097964"}},"outputId":"cecd1cd0-8081-404e-830d-0adfa9b7f603"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 160, 160)          4705120   \n","                                                                 \n"," spatial_dropout1d (SpatialD  (None, 160, 160)         0         \n"," ropout1D)                                                       \n","                                                                 \n"," lstm (LSTM)                 (None, 250)               411000    \n","                                                                 \n"," dense (Dense)               (None, 5)                 1255      \n","                                                                 \n","=================================================================\n","Total params: 5,117,375\n","Trainable params: 5,117,375\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}],"source":["# Model summary \n","\n","embed_dim = 160\n","lstm_out = 250\n","\n","model_text = Sequential()\n","model_text.add(Embedding(len(w_idx) +1 , embed_dim,input_length = X_test_pad.shape[1]))\n","model_text.add(SpatialDropout1D(0.2))\n","model_text.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n","model_text.add(keras.layers.core.Dense(5, activation='softmax'))\n","#adam rmsprop \n","model_text.compile(loss = \"categorical_crossentropy\", optimizer='adam',metrics = ['accuracy'])\n","print(model_text.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Iig0nuWNRYh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e6fb2534-2de8-43fb-a91b-9fef695bf1f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n"," 122/1014 [==>...........................] - ETA: 22:29 - loss: 1.3679 - accuracy: 0.3622"]}],"source":["# Fitting Model \n","model_text.fit(X_train_pad, y_train, epochs = Epoch, batch_size=batch_size,validation_data=(X_test_pad, y_test))"]},{"cell_type":"code","source":["# Confusion Matrix on Validation Set for LSTM Model\n","\n","num_of_classess = 5\n","\n","prediction_text = (model_text.predict(X_test_pad))\n","\n","a = np.argmax(y_test, axis=1)\n","b = np.argmax(prediction_text, axis=1)\n","\n","labels = ['Sad', 'Happy', 'Neutral', 'Surprise','Anger']\n","cm = confusion_matrix(a,b )\n","\n","plt.figure(figsize = (10,10))\n","plt.title(\"Heatmap for Validation set\")\n","sns.heatmap(cm, annot=True, xticklabels =  labels,yticklabels = labels, annot_kws={\"size\": 12},fmt='g',cmap ='Paired') # font size \n","plt.show()"],"metadata":{"id":"KTG0-MHzixUa"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"FHh0YsvMNRYk"},"outputs":[],"source":["# Testing on Sentences\n","testing = \"I am happy with my life\"\n","result =get_sentiment(model_text,testing)\n","print(f'Sentence : {testing} \\n {result} {plot_result(result)}')\n"]},{"cell_type":"markdown","metadata":{"id":"TivnFQawNRYl"},"source":["## LSTM with Glove "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0eJIuPt5NRYm"},"outputs":[],"source":["# https://nlp.stanford.edu/projects/glove/\n","# Downloaded the word embeddings from glove website \n","# Function to read the GLOVE file and find the words  \n","def read_data(file_name):\n","    with open(file_name,'r') as f:\n","        word_vocab = set() \n","        word2vector = {}\n","        for line in f:\n","            line_ = line.strip() \n","            words_Vec = line_.split()\n","            word_vocab.add(words_Vec[0])\n","            word2vector[words_Vec[0]] = np.array(words_Vec[1:],dtype=float)\n","    print(\"Total Words in DataSet:\",len(word_vocab))\n","    return word_vocab,word2vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4kQVQf61NRYn"},"outputs":[],"source":["# Loading the Glove file \n","\n","vocab, word_to_idx =read_data(\"/content/drive/MyDrive/Emotional_messenger/text/glove.twitter.27B.200d.txt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FYQPYbaDNRYo"},"outputs":[],"source":["# Embedding the sentences with 0's\n","embedding_matrix = np.zeros((len(w_idx) + 1, 200))\n","for word, i in w_idx.items():\n","    embedding_vector = word_to_idx.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector"]},{"cell_type":"code","source":["embed_dim = 200\n","lstm_out = 250\n","\n","model_lstm_gwe = Sequential()\n","model_lstm_gwe.add(Embedding(len(w_idx) +1 , embed_dim,input_length = X_test_pad.shape[1],weights=[embedding_matrix],trainable=False))\n","model_lstm_gwe.add(SpatialDropout1D(0.2))\n","model_lstm_gwe.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n","model_lstm_gwe.add(keras.layers.core.Dense(5, activation='softmax'))\n","#adam rmsprop \n","model_lstm_gwe.compile(loss = \"categorical_crossentropy\", optimizer='adam',metrics = ['accuracy'])\n","print(model_lstm_gwe.summary())"],"metadata":{"id":"DbLUgyucW8-T"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"pTSjMfKYNRYq"},"outputs":[],"source":["# Fitting Model \n","model_lstm_gwe.fit(X_train_pad, y_train, epochs = Epoch, batch_size=batch_size,validation_data=(X_test_pad, y_test))"]},{"cell_type":"code","source":["# Testing on Sentences\n","testing = \"I am happy with my life\"\n","result =get_sentiment(model_lstm_gwe,testing)\n","print(f'Sentence : {testing} \\n {result} {plot_result(result)}')\n"],"metadata":{"id":"lvqVDPvfho5L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Confusion Matrix on Validation Set for LSTM + Glove Model\n","\n","num_of_classess = 5\n","\n","prediction_text = (model_lstm_gwe.predict(X_test_pad))\n","\n","a = np.argmax(y_test, axis=1)\n","b = np.argmax(prediction_text, axis=1)\n","\n","labels = ['Sad', 'Happy', 'Neutral', 'Surprise','Anger']\n","cm = confusion_matrix(a,b )\n","\n","plt.figure(figsize = (10,10))\n","plt.title(\"Heatmap for Validation set\")\n","sns.heatmap(cm, annot=True, xticklabels =  labels,yticklabels = labels, annot_kws={\"size\": 12},fmt='g',cmap ='Paired') # font size \n","plt.show()"],"metadata":{"id":"mcoslAoehClc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L-rotEyoNRYs"},"source":["#### Saving the Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uBAJYqsaNRYt"},"outputs":[],"source":["# Saving LSTM Model \n","\n","model_text.save('/content/drive/MyDrive/Emotional_messenger/text/saved_model/my_model')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pUIFqtgNNRYu"},"outputs":[],"source":["# Saving LSTM+Glove Model \n","\n","model_lstm_gwe.save('/content/drive/MyDrive/Emotional_messenger/text/saved_model_glove/my_model_glove')"]},{"cell_type":"markdown","metadata":{"id":"4l1Mz52FNRYv"},"source":["#### Loading saved models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-H3QatJjNRYv"},"outputs":[],"source":["# Loading Saved Model \n","\n","# model_text = tf.keras.models.load_model('/content/drive/MyDrive/Emotional_messenger/text/saved_model/my_model')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hoLKXlUuNRYw"},"outputs":[],"source":["# Loading LSTM+Glove Model \n","\n","# model_lstm_gwe = tf.keras.models.load_model('/content/drive/MyDrive/Emotional_messenger/text/saved_model_glove/my_model_glove')"]},{"cell_type":"code","source":["# Final Testing of Loaded Model"],"metadata":{"id":"BlWAfSV2cAE3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"F7rkn4_ENRYx"},"outputs":[],"source":["# Testing the model loaded, Glove is having higher accuracy so we will be using GLove model\n","\n","# Testing on Sentences\n","testing = \"I hate Vancouver\"\n","result =get_sentiment(model_lstm_gwe,testing)\n","print(f'Sentence : {testing} \\n {result} {plot_result(result)}')\n","\n","\n"]},{"cell_type":"markdown","source":["## Phase II - Emotion detection using Image dataset FER2013"],"metadata":{"id":"dL5UcFdmXphJ"}},{"cell_type":"code","source":["# Installing Dependencies\n","\n","!pip install opencv-python-headless==4.1.2.30\n","!pip install SpeechRecognition\n","!pip install comtypes\n","!pip3 install gTTS pyttsx3 playsound                                              # https://github.com/nateshmbhat/pyttsx3\n","!apt-get update && sudo apt-get install espeak"],"metadata":{"id":"S8VQr4FeV9kW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import dependencies for Sound and Speech Output\n","from IPython.display import display, Javascript, Image                            # https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html\n","from google.colab.output import eval_js                                           # https://colab.research.google.com/notebooks/snippets/advanced_outputs.ipynb\n","from base64 import b64decode, b64encode\n","from playsound import playsound                                                   # https://github.com/TaylorSMarks/playsound\n","import cv2                                                                        # https://github.com/opencv/opencv-python\n","\n","import os\n","import time\n","import PIL\n","import io\n","import html\n","import pyttsx3\n","import datetime\n","import random\n","import imageio \n","\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'retina'\n","sns.set_style('white')\n","\n","# Model loading and Training\n","from PIL import Image\n","from skimage import io                                                            # https://scikit-image.org/\n","from sklearn import svm\n","from sklearn.model_selection import cross_validate, KFold\n","from sklearn.metrics import recall_score,precision_score,f1_score,accuracy_score  # https://scikit-learn.org/stable/\n","from sklearn.mixture import GaussianMixture\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import ConfusionMatrixDisplay\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator               # https://www.tensorflow.org/\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.models import load_model, model_from_json\n","from tensorflow.keras.preprocessing.image import load_img,img_to_array\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.models import Model\n","from keras.layers.convolutional import Conv2D                                     # https://keras.io/\n","from keras.layers.pooling import MaxPooling2D\n","from keras.layers.merge import concatenate\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.regularizers import l1, l2\n","\n"],"metadata":{"id":"aRRS83MrV96P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading Dataset - Train and test\n","\n","df = pd.read_csv('/content/drive/MyDrive/Emotional_messenger/face/train.csv')\n","df.columns = [col.replace(\" \", \"\") for col in df.columns]\n","df_test = pd.read_csv('/content/drive/MyDrive/Emotional_messenger/face/test.csv')\n","training_data, testing_data = train_test_split(df, test_size=0.2, random_state=25)\n"],"metadata":{"id":"62lM0QMaV-Lt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Splitting dataset in columns\n","\n","X_train = []\n","y_train = []\n","for index, row in training_data.iterrows():\n","    k = row['pixels'].split(\" \")\n","    \n","    X_train.append(np.array(k))\n","    y_train.append(row['emotion'])\n","X_test = []\n","y_test = []\n","for index, row in testing_data.iterrows():\n","    k = row['pixels'].split(\" \")\n","    \n","    X_test.append(np.array(k))\n","    y_test.append(row['emotion'])\n","    \n","    "],"metadata":{"id":"CvIsLK8zV-uM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Converting to integer type values\n","\n","X_train = np.array(X_train, dtype = 'uint8')\n","y_train = np.array(y_train, dtype = 'uint8')\n","X_test = np.array(X_test, dtype = 'uint8')\n","y_test = np.array(y_test, dtype = 'uint8')"],"metadata":{"id":"tCfYDTXqV_ni"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Converting labels to categorical values and training data to tensor\n","y_train= to_categorical(y_train, num_classes=7)\n","y_test = to_categorical(y_test, num_classes=7)\n","X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n","X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)"],"metadata":{"id":"2WWe1Ie9t6J2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extending dataset with data augmentation \n","datagen = ImageDataGenerator( \n","    rescale=1./255,\n","    rotation_range = 10,\n","    horizontal_flip = True,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    fill_mode = 'nearest')\n","testgen = ImageDataGenerator(rescale=1./255)\n","datagen.fit(X_train)\n","batch_size = 64\n","\n","train_flow = datagen.flow(X_train, y_train, batch_size=batch_size) \n","test_flow = testgen.flow(X_test, y_test, batch_size=batch_size)"],"metadata":{"id":"1R4VVdpht6B3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating final CNN Model\n","\n","def FER_Model(input_shape=(48,48,1)):\n","    # first input model\n","    visible = Input(shape=input_shape, name='input')\n","    num_classes = 7\n","    #the 1-st block\n","    conv1_1 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_1')(visible)\n","    conv1_1 = BatchNormalization()(conv1_1)\n","    conv1_2 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_2')(conv1_1)\n","    conv1_2 = BatchNormalization()(conv1_2)\n","    pool1_1 = MaxPooling2D(pool_size=(2,2), name = 'pool1_1')(conv1_2)\n","    drop1_1 = Dropout(0.3, name = 'drop1_1')(pool1_1)#the 2-nd block\n","    conv2_1 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_1')(drop1_1)\n","    conv2_1 = BatchNormalization()(conv2_1)\n","    conv2_2 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_2')(conv2_1)\n","    conv2_2 = BatchNormalization()(conv2_2)\n","    conv2_3 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_3')(conv2_2)\n","    conv2_2 = BatchNormalization()(conv2_3)\n","    pool2_1 = MaxPooling2D(pool_size=(2,2), name = 'pool2_1')(conv2_3)\n","    drop2_1 = Dropout(0.3, name = 'drop2_1')(pool2_1)#the 3-rd block\n","    conv3_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_1')(drop2_1)\n","    conv3_1 = BatchNormalization()(conv3_1)\n","    conv3_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_2')(conv3_1)\n","    conv3_2 = BatchNormalization()(conv3_2)\n","    conv3_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_3')(conv3_2)\n","    conv3_3 = BatchNormalization()(conv3_3)\n","    conv3_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_4')(conv3_3)\n","    conv3_4 = BatchNormalization()(conv3_4)\n","    pool3_1 = MaxPooling2D(pool_size=(2,2), name = 'pool3_1')(conv3_4)\n","    drop3_1 = Dropout(0.3, name = 'drop3_1')(pool3_1)#the 4-th block\n","    conv4_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_1')(drop3_1)\n","    conv4_1 = BatchNormalization()(conv4_1)\n","    conv4_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_2')(conv4_1)\n","    conv4_2 = BatchNormalization()(conv4_2)\n","    conv4_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_3')(conv4_2)\n","    conv4_3 = BatchNormalization()(conv4_3)\n","    conv4_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_4')(conv4_3)\n","    conv4_4 = BatchNormalization()(conv4_4)\n","    pool4_1 = MaxPooling2D(pool_size=(2,2), name = 'pool4_1')(conv4_4)\n","    drop4_1 = Dropout(0.3, name = 'drop4_1')(pool4_1)\n","    \n","    #the 5-th block\n","    conv5_1 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_1')(drop4_1)\n","    conv5_1 = BatchNormalization()(conv5_1)\n","    conv5_2 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_2')(conv5_1)\n","    conv5_2 = BatchNormalization()(conv5_2)\n","    conv5_3 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_3')(conv5_2)\n","    conv5_3 = BatchNormalization()(conv5_3)\n","    conv5_4 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_4')(conv5_3)\n","    conv5_3 = BatchNormalization()(conv5_3)\n","    pool5_1 = MaxPooling2D(pool_size=(2,2), name = 'pool5_1')(conv5_4)\n","    drop5_1 = Dropout(0.3, name = 'drop5_1')(pool5_1)#Flatten and output\n","    flatten = Flatten(name = 'flatten')(drop5_1)\n","    ouput = Dense(num_classes, activation='softmax', name = 'output')(flatten)# create model \n","    model = Model(inputs =visible, outputs = ouput)\n","    # summary layers\n","    print(model.summary())\n","    \n","    return model"],"metadata":{"id":"bPWWWZevt55z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model definition\n","model_face = FER_Model()\n","opt = Adam(lr=0.0001, decay=1e-6)\n","model_face.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"],"metadata":{"id":"E1aMxDeAt5yY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training Model \n","\n","num_epochs =   50\n","history = model_face.fit_generator(train_flow, \n","                    steps_per_epoch=len(X_train) / batch_size, \n","                    epochs=num_epochs,  \n","                    verbose=1,  \n","                    validation_data=test_flow,\n","                    validation_steps=len(X_test) / batch_size)"],"metadata":{"id":"FOQoBa5at5n3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving model \n","\n","model_face.save('/content/drive/MyDrive/Emotional_messenger/face/saved_model/my_model')"],"metadata":{"id":"y8G0oLuCt5fz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading Model\n","\n","# model_face = tf.keras.models.load_model('/content/drive/MyDrive/Emotional_messenger/face/saved_model/my_model')"],"metadata":{"id":"rn_pzhAUt5Xq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"chVkZiWmNRY1"},"source":["####Face Recognition using Javascript"]},{"cell_type":"code","source":["# Creating object for taking photo and video in the frame\n","\n","def take_photo(filename='photo.jpg', quality=0.8):\n","  js = Javascript('''\n","    async function takePhoto(quality) {\n","      const div = document.createElement('div');\n","      const capture = document.createElement('button');\n","      capture.textContent = 'Capture';\n","      div.appendChild(capture);\n","\n","      const video = document.createElement('video');\n","      video.style.display = 'block';\n","      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n","\n","      document.body.appendChild(div);\n","      div.appendChild(video);\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      // Resize the output to fit the video element.\n","      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","      // Wait for Capture to be clicked.\n","      await new Promise((resolve) => capture.onclick = resolve);\n","\n","      const canvas = document.createElement('canvas');\n","      canvas.width = video.videoWidth;\n","      canvas.height = video.videoHeight;\n","      canvas.getContext('2d').drawImage(video, 0, 0);\n","      stream.getVideoTracks()[0].stop();\n","      div.remove();\n","      return canvas.toDataURL('image/jpeg', quality);\n","    }\n","    ''')\n","  display(js)\n","  data = eval_js('takePhoto({})'.format(quality))\n","  binary = b64decode(data.split(',')[1])\n","  with open(filename, 'wb') as f:\n","    f.write(binary)\n","  return filename"],"metadata":{"id":"sLv5O6N30PMm"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ghUlAJzKSjFT"},"source":["# JavaScript to properly create our live video stream using our webcam as input\n","def video_stream():\n","  js = Javascript('''\n","    var video;\n","    var div = null;\n","    var stream;\n","    var captureCanvas;\n","    var imgElement;\n","    var labelElement;\n","    var textdata;\n","    var pendingResolve = null;\n","    var shutdown = false;\n","    \n","    function removeDom() {\n","       stream.getVideoTracks()[0].stop();\n","       video.remove();\n","       div.remove();\n","       video = null;\n","       div = null;\n","       stream = null;\n","       imgElement = null;\n","       captureCanvas = null;\n","       labelElement = null;\n","    }\n","    \n","    function onAnimationFrame() {\n","      if (!shutdown) {\n","        window.requestAnimationFrame(onAnimationFrame);\n","      }\n","      if (pendingResolve) {\n","        var result = \"\";\n","        if (!shutdown) {\n","          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n","          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n","        }\n","        var lp = pendingResolve;\n","        pendingResolve = null;\n","        lp(result);\n","      }\n","    }\n","    \n","    async function createDom() {\n","      if (div !== null) {\n","        return stream;\n","      }\n","\n","      div = document.createElement('div');\n","      div.style.border = '2px solid black';\n","      div.style.padding = '3px';\n","      div.style.width = '100%';\n","      div.style.maxWidth = '600px';\n","      document.body.appendChild(div);\n","      \n","      const modelOut = document.createElement('div');\n","      modelOut.innerHTML = \"<span>Status:</span>\";\n","      labelElement = document.createElement('span');\n","      labelElement.innerText = 'No data';\n","      labelElement.style.fontWeight = 'bold';\n","      modelOut.appendChild(labelElement);\n","      div.appendChild(modelOut);\n","           \n","      video = document.createElement('video');\n","      video.style.display = 'block';\n","      video.width = div.clientWidth - 6;\n","      video.setAttribute('playsinline', '');\n","      video.onclick = () => { shutdown = true; };\n","      stream = await navigator.mediaDevices.getUserMedia(\n","          {video: { facingMode: \"environment\"}});\n","      div.appendChild(video);\n","\n","      imgElement = document.createElement('img');\n","      imgElement.style.position = 'absolute';\n","      imgElement.style.zIndex = 1;\n","      imgElement.onclick = () => { shutdown = true; };\n","      div.appendChild(imgElement);\n","      \n","      \n","      \n","     \n","      \n","      const instruction = document.createElement('div');\n","      instruction.innerHTML = \n","          '<button type=\"button\">Send Text!</button>';\n","      \n","      instruction.onclick = () => {shutdown = true; };\n","      div.appendChild(instruction);\n","      video.srcObject = stream;\n","      await video.play();\n","\n","      captureCanvas = document.createElement('canvas');\n","      captureCanvas.width = 640; //video.videoWidth;\n","      captureCanvas.height = 480; //video.videoHeight;\n","      window.requestAnimationFrame(onAnimationFrame);\n","      \n","      return stream;\n","    }\n","    async function stream_frame(label, imgData) {\n","      if (shutdown) {\n","        removeDom();\n","        shutdown = false;\n","        return '';\n","      }\n","\n","      var preCreate = Date.now();\n","      stream = await createDom();\n","      \n","      var preShow = Date.now();\n","      if (label != \"\") {\n","        labelElement.innerHTML = label;\n","      }\n","            \n","      if (imgData != \"\") {\n","        var videoRect = video.getClientRects()[0];\n","        imgElement.style.top = videoRect.top + \"px\";\n","        imgElement.style.left = videoRect.left + \"px\";\n","        imgElement.style.width = videoRect.width + \"px\";\n","        imgElement.style.height = videoRect.height + \"px\";\n","        imgElement.src = imgData;\n","      }\n","      \n","      var preCapture = Date.now();\n","      var result = await new Promise(function(resolve, reject) {\n","        pendingResolve = resolve;\n","      });\n","      shutdown = false;\n","      \n","      return {'create': preShow - preCreate, \n","              'show': preCapture - preShow, \n","              'capture': Date.now() - preCapture,\n","              'img': result\n","              };\n","    }\n","    ''')\n","\n","  display(js)\n","#  return(JSON.stringify(js))\n","  \n","def video_frame(label, bbox):\n","  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# function to convert the JavaScript object into an OpenCV image #https://jfine-python-classes.readthedocs.io/en/latest/javascript-objects.html\n","# https://docs.opencv.org/3.4/d5/d10/tutorial_js_root.html\n","\n","def js_to_image(js_reply):\n","  \"\"\"\n","  Params:\n","          js_reply: JavaScript object containing image from webcam\n","  Returns:\n","          img: OpenCV BGR image\n","  \"\"\"\n","  # decode base64 image\n","  image_bytes = b64decode(js_reply.split(',')[1])\n","  # convert bytes to numpy array\n","  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n","  # decode numpy array into OpenCV BGR image\n","  img = cv2.imdecode(jpg_as_np, flags=1)\n","\n","  return img\n","\n","# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n","def bbox_to_bytes(bbox_array):\n","  \"\"\"\n","  Params:\n","          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n","  Returns:\n","        bytes: Base64 image byte string\n","  \"\"\"\n","  # convert array into PIL image\n","  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n","  iobuf = io.BytesIO()\n","  # format bbox into png for return\n","  bbox_PIL.save(iobuf, format='png')\n","  # format return string\n","  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n","\n","  return bbox_bytes"],"metadata":{"id":"dYIQNOg470GF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Main function to read video and converting it to csv using opencv and then predicting the emotion using model_face\n","video_stream()\n","\n","totalprob=[]\n","# label for video\n","label_html = 'Capturing...'\n","# initialze bounding box to empty\n","bbox = ''\n","counter = 0 \n","count=0\n","model = model_face\n","# model.load_weights('model.h5')\n","# model = load_model('static\\Fer2013.h5')\n","face_haar_cascade = cv2.CascadeClassifier(cv2.samples.findFile(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'))\n","t_end = time.time() + 10\n","while time.time() < t_end:\n","    js_reply = video_frame(label_html, bbox)\n","    \n","    \n","    if not js_reply:\n","        break\n","    \n","    # convert JS response to OpenCV Image\n","    img = js_to_image(js_reply[\"img\"])\n","    \n","    if (counter==0):\n","      val = input(\"Enter the text message: \")\n","      print(val)\n","      counter+=1\n","    \n","    height, width , channel = img.shape\n","    sub_img = img[0:int(height/6),0:int(width)]\n","    bbox_array = np.ones(sub_img.shape, dtype=np.uint8)*0\n","    black_rect = np.ones(sub_img.shape, dtype=np.uint8)*0\n","    res = cv2.addWeighted(sub_img, 0.77, bbox_array,0.23, 0)\n","    FONT = cv2.FONT_HERSHEY_SIMPLEX\n","    FONT_SCALE = 0.8\n","    FONT_THICKNESS = 2\n","    lable_color = (10, 10, 255)\n","    lable = \"Emotion Detection made by Nikita & Animesh\"\n","    lable_dimension = cv2.getTextSize(lable,FONT ,FONT_SCALE,FONT_THICKNESS)[0]\n","    textX = int((res.shape[1] - lable_dimension[0]) / 2)\n","    textY = int((res.shape[0] + lable_dimension[1]) / 2)\n","    cv2.putText(res, lable, (textX,textY), FONT, FONT_SCALE, (0,0,0), FONT_THICKNESS)\n","    gray_image= cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    faces = face_haar_cascade.detectMultiScale(gray_image )\n","    for (x,y,w,h) in faces:\n","      cv2.rectangle(bbox_array, pt1 = (x,y),pt2 = (x+w, y+h), color = (255,0,0),thickness =  2)\n","      roi_gray = gray_image[y-5:y+h+5,x-5:x+w+5]\n","      roi_gray=cv2.resize(roi_gray,(48,48))\n","      image_pixels = img_to_array(roi_gray)\n","      image_pixels = np.expand_dims(image_pixels, axis = 0)\n","      image_pixels /= 255\n","      predictions = model.predict(image_pixels)\n","      totalprob.append(predictions)\n","      max_index = np.argmax(predictions[0])\n","      #print(totalprob)\n","      emotion_detection = ('angry', 'angry', 'surprise', 'happy', 'sad', 'surprise', 'neutral')\n","      emotion_prediction = emotion_detection[max_index]\n","      \n","      #print(predictions)\n","      cv2.putText(res, \"Sentiment: {}\".format(emotion_prediction), (0,textY+22+5), FONT,0.7, lable_color,2)\n","      lable_violation = 'Confidence: {}'.format(str(np.round(np.max(predictions[0])*100,1))+ \"%\")\n","      violation_text_dimension = cv2.getTextSize(lable_violation,FONT,FONT_SCALE,FONT_THICKNESS )[0]\n","      violation_x_axis = int(res.shape[1]- violation_text_dimension[0])\n","      cv2.putText(res, lable_violation, (violation_x_axis,textY+22+5), FONT,0.7, lable_color,2)\n","    #pp=(js_video)\n","    bbox_array[0:int(height/6),0:int(width)] = res\n","    \n","    # convert overlay of bbox into bytes\n"," \n","\n","# print(predictions)\n","label_html1 = 'Capturing completed'\n","js_reply = video_frame(label_html1, bbox)\n","cv2.destroyAllWindows\n"],"metadata":{"id":"Sohh3RYg7fMq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating a zero list for saving result\n","av=[0,0,0,0,0,0,0]\n","#print(len(totalprob))\n","for i in range(0,len(totalprob)):\n","  for j in range(0,7):\n","    av[j]=totalprob[i][0][j]+av[j]\n","for k in range(0,len(av)):\n","  av[k]=av[k]/(len(totalprob))\n","emotion_detection = ('angry','happy', 'sad', 'surprise', 'neutral')\n","del av[1]\n","del av[1]\n","\n","max_index = np.argmax(av)\n","dfemotion = pd.DataFrame(av)\n","dfemotion['label']=emotion_detection\n","print(f' Face Emotion Detection result : {emotion_detection[max_index]}')\n","result_camera = dfemotion.copy()\n"],"metadata":{"id":"ENJENyr7Sbju"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h84yEtbjNRY2"},"source":["## Phase III - Adding both Text and Face Analysis to generate the resulting Emotion and narrating the output"]},{"cell_type":"code","source":["# Finding result on text sentence using LSTM + Glove Model and Video output using CNN Model\n","\n","result_text = get_sentiment(model_lstm_gwe,val)\n","result_face = result_camera.copy()\n","\n","# Creating a new DataFrame to store final result\n","final_output = result_text"],"metadata":{"id":"42ZAWiD4VaPK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(result_text , result_face)"],"metadata":{"id":"GpXwDUI5hg6A"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MYIvM5t7NRY4"},"outputs":[],"source":["# Adding Result for both the models with a weightage of Face = 0.8 and Text = 0.2 percentage\n","\n","import numpy as np\n","sum = []\n","def Average(r1,r2):\n","    for i in range(len(r1)):\n","        a = ( (r1.loc[i][1]) * 0.2 + ((r2.loc[i][0]*100)) * 1.8 ) /2\n","        sum.append(a)\n","    return sum\n","\n","\n","res = Average(result_text,result_face) \n","\n","k = 0\n","for i in res:\n","    final_output.iloc[k][1] = res[k]\n","    k+=1\n","# final_output"]},{"cell_type":"code","source":["# Final Emotion from both the Models\n","choose=final_output.loc[final_output['percentage']==final_output['percentage'].max()]\n","print(f' Choosen Emotion from Both Models : {choose.sentiment}')\n"],"metadata":{"id":"JyjTsToIfpJi"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zItrx-giNRY5"},"outputs":[],"source":["# Modified plotting function for 1 entry\n","def plot_result1(df):\n","    #colors=['#D50000','#000000','#008EF8','#F5B27B','#EDECEC','#D84A09','#019BBD','#FFD000','#7800A0','#098F45','#807C7C','#85DDE9','#F55E10']\n","\n","    labels = ['Sad', 'Happy', 'Neutral', 'Surprise','Anger']\n","    colors={'sad':'rgb(64,224,208)','happy':'rgb(100,149,237)',\n","                    'neutral':'rgb(138,43,226)','surprise':'rgb(221,160,221)',\n","                    'anger':'rgb(255,218,185)'}\n","    col_2={}\n","    for i in final_output.sentiment.to_list():\n","        col_2[i]=colors[i]\n","    fig = px.pie(df, values='percentage', names='sentiment',color='sentiment',color_discrete_map=col_2,hole=0.3)\n","\n","    fig.show()"]},{"cell_type":"markdown","source":["# Result : Emotion Pie Chart and Narrated Output"],"metadata":{"id":"X-BJD8XFkcNg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DWfX5x6rNRY6"},"outputs":[],"source":["print(f'Sentence : {val}. \\n Emotion :{choose.sentiment} {plot_result1(final_output)}')"]},{"cell_type":"markdown","metadata":{"id":"R3o1pZQeNRY7"},"source":["### Output with tone of the resulting emotion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dy0NdLV4NRY8"},"outputs":[],"source":["# Playing the text in speech with tone # https://librosa.org/doc/latest/index.html\n","\n","import IPython.display as ipd\n","import gtts\n","\n","# make request to google to get synthesis\n","tts = gtts.gTTS(val)\n","\n","# save the audio file\n","tts.save(\"/content/drive/MyDrive/Emotional_messenger/string.mp3\")\n","ipd.Audio('/content/drive/MyDrive/Emotional_messenger/string.mp3') # load a local WAV file"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"emotional_messenger.ipynb","provenance":[{"file_id":"1RfT1j82oIEwIEOlkMv14MjF75KFCLjq7","timestamp":1649814149326}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}